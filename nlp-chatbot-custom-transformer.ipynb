{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nimport pandas as pd\nimport torch\nimport random\nimport torch.nn as nn\nimport transformers\nimport matplotlib.pyplot as plt\n# # specify GPU\n# device = torch.device(“cuda”)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T10:47:52.335702Z","iopub.execute_input":"2023-01-26T10:47:52.336143Z","iopub.status.idle":"2023-01-26T10:47:52.342524Z","shell.execute_reply.started":"2023-01-26T10:47:52.336104Z","shell.execute_reply":"2023-01-26T10:47:52.341566Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# config params\nVOCAB_SIZE = 10000 #8500\nOUTPUT_SEQUENCE_LENGTH = 50 #40\nMAX_SAMPLES = 80000 #5000\nBUFFER_SIZE = 2048\nBATCH_SIZE = 64\n# EPOCHS = 2\nEMBED_DIM = 256\nLATENT_DIM = 1024\nNUM_HEADS = 8\nSTART_TOKEN = '[start]'\nSTOP_TOKEN = '[stop]'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T10:47:55.975998Z","iopub.execute_input":"2023-01-26T10:47:55.976348Z","iopub.status.idle":"2023-01-26T10:47:55.982028Z","shell.execute_reply.started":"2023-01-26T10:47:55.976319Z","shell.execute_reply":"2023-01-26T10:47:55.981009Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def load_conversations(max_samples:int=50000) -> list:\n    def clean_text(input_text:str) -> str:\n        res = input_text.lower().strip()\n        res = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", res)\n        return res\n        \n    sep = ' +++$+++ '\n\n    movie_lines = {}\n    with open('/kaggle/input/cornell-moviedialog-corpus/movie_lines.txt', 'r', encoding='iso-8859-1') as f:\n        lines = f.read().split('\\n')\n    for line in lines:\n        key = line.split(sep)[0]\n        value = line.split(sep)[-1]\n        movie_lines[key] = value\n    \n    line_pairs = []\n    with open('/kaggle/input/cornell-moviedialog-corpus/movie_conversations.txt', 'r', encoding='iso-8859-1') as f:\n        lines = f.read().split('\\n')\n    for line in lines:\n        conversation = line.split(sep)[-1][1:-2].replace(\"'\", '').split(', ')\n        for i in range(len(conversation) - 1):\n            statement = clean_text(movie_lines[conversation[i]])\n            response = clean_text(movie_lines[conversation[i + 1]])\n            response = START_TOKEN + ' ' + response + ' ' + STOP_TOKEN\n            line_pairs.append((statement, response))\n            if len(line_pairs) >= max_samples:\n                return line_pairs\n    return line_pairs\n\nline_pairs = load_conversations(MAX_SAMPLES)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T10:48:07.095928Z","iopub.execute_input":"2023-01-26T10:48:07.096298Z","iopub.status.idle":"2023-01-26T10:48:08.132618Z","shell.execute_reply.started":"2023-01-26T10:48:07.096265Z","shell.execute_reply":"2023-01-26T10:48:08.131566Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def split_train_test_data(dataset:list, test_size:float=0.1) -> tuple:\n    np.random.shuffle(dataset)\n    split_idx = int(len(dataset) * (1 - test_size))\n    train_ds = dataset[:split_idx]\n    valid_ds = dataset[split_idx:]\n    return train_ds, valid_ds\n\ntrain_pairs, valid_pairs = split_train_test_data(line_pairs)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T10:48:09.885990Z","iopub.execute_input":"2023-01-26T10:48:09.886343Z","iopub.status.idle":"2023-01-26T10:48:09.922212Z","shell.execute_reply.started":"2023-01-26T10:48:09.886313Z","shell.execute_reply":"2023-01-26T10:48:09.919784Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def get_vectorizer(dataset:list) -> tuple:\n    input_vectorizer = layers.TextVectorization(\n        VOCAB_SIZE,\n        output_sequence_length=OUTPUT_SEQUENCE_LENGTH,\n        standardize=None\n    )\n    target_vectorizer = layers.TextVectorization(\n        VOCAB_SIZE,\n        output_sequence_length=OUTPUT_SEQUENCE_LENGTH + 1,\n        standardize=None\n    )\n    statements, responses = zip(*dataset)\n    input_vectorizer.adapt(list(statements))\n    target_vectorizer.adapt(list(responses))\n    return input_vectorizer, target_vectorizer\n\ninput_vectorizer, target_vectorizer = get_vectorizer(line_pairs)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T10:48:16.873492Z","iopub.execute_input":"2023-01-26T10:48:16.873871Z","iopub.status.idle":"2023-01-26T10:48:28.256007Z","shell.execute_reply.started":"2023-01-26T10:48:16.873816Z","shell.execute_reply":"2023-01-26T10:48:28.254885Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def create_dataset(dataset:list):\n    def vectorize_text(statements, responses):\n        inputs, outputs = input_vectorizer(statements), target_vectorizer(responses)\n        return (\n            {\"encoder_inputs\": inputs, \"decoder_inputs\": outputs[:, :-1]},\n            {\"outputs\": outputs[:, 1:]}\n        )\n\n    statements, responses = zip(*dataset)\n    dataset = tf.data.Dataset.from_tensor_slices((list(statements), list(responses)))\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.map(vectorize_text)\n    return dataset.shuffle(BUFFER_SIZE).prefetch(16).cache()\n\ntrain_ds = create_dataset(train_pairs)\nvalid_ds = create_dataset(valid_pairs)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T10:48:28.259067Z","iopub.execute_input":"2023-01-26T10:48:28.259492Z","iopub.status.idle":"2023-01-26T10:48:29.210402Z","shell.execute_reply.started":"2023-01-26T10:48:28.259443Z","shell.execute_reply":"2023-01-26T10:48:29.209372Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Tranformer models\n# Custom transformer\n\nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads):\n        super(TransformerEncoder, self).__init__()\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential([\n            layers.Dense(dense_dim, activation=\"relu\"), \n            layers.Dense(embed_dim)\n        ])\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n            \n        attention_output = self.attention(\n            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n        )\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T10:48:29.212216Z","iopub.execute_input":"2023-01-26T10:48:29.212984Z","iopub.status.idle":"2023-01-26T10:48:29.221913Z","shell.execute_reply.started":"2023-01-26T10:48:29.212947Z","shell.execute_reply":"2023-01-26T10:48:29.220733Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim):\n        super(PositionalEmbedding, self).__init__()\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(length)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T10:48:29.225086Z","iopub.execute_input":"2023-01-26T10:48:29.225789Z","iopub.status.idle":"2023-01-26T10:48:29.237500Z","shell.execute_reply.started":"2023-01-26T10:48:29.225735Z","shell.execute_reply":"2023-01-26T10:48:29.236538Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads):\n        super(TransformerDecoder, self).__init__()\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential([\n            layers.Dense(latent_dim, activation=\"relu\"), \n            layers.Dense(embed_dim)\n        ])\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(\n            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n        )\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(\n            query=out_1, value=encoder_outputs,key=encoder_outputs,\n            attention_mask=padding_mask\n        )\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n        proj_output = self.dense_proj(out_2)\n        return self.layernorm_3(out_2 + proj_output)\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        print(i, j)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T10:48:29.238972Z","iopub.execute_input":"2023-01-26T10:48:29.239332Z","iopub.status.idle":"2023-01-26T10:48:29.255656Z","shell.execute_reply.started":"2023-01-26T10:48:29.239299Z","shell.execute_reply":"2023-01-26T10:48:29.254759Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def create_model() -> keras.Model:\n    # encoder\n    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n    encoder_augmented_inputs = PositionalEmbedding(OUTPUT_SEQUENCE_LENGTH, VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n    encoder_outputs = TransformerEncoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(encoder_augmented_inputs)\n\n    # decoder\n    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n    encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n    decoder_augmented_inputs = PositionalEmbedding(OUTPUT_SEQUENCE_LENGTH, VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n    decoder_outputs = TransformerDecoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(decoder_augmented_inputs, encoded_seq_inputs)\n    decoder_outputs = layers.Dropout(0.5)(decoder_outputs)\n    decoder_outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(decoder_outputs)\n\n    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name='outputs')\n    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n\n    model = keras.Model(\n        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n    )\n\n    model.compile(\n        \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n    )\n\n    return model\n\nmodel = create_model()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-26T10:48:29.257212Z","iopub.execute_input":"2023-01-26T10:48:29.257576Z","iopub.status.idle":"2023-01-26T10:48:29.798295Z","shell.execute_reply.started":"2023-01-26T10:48:29.257542Z","shell.execute_reply":"2023-01-26T10:48:29.797010Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Tensor(\"transformer_decoder_2/strided_slice_2:0\", shape=(None, 1), dtype=int32) Tensor(\"transformer_decoder_2/range_1:0\", shape=(None,), dtype=int32)\nTensor(\"outputs/transformer_decoder_2/strided_slice_2:0\", shape=(None, 1), dtype=int32) Tensor(\"outputs/transformer_decoder_2/range_1:0\", shape=(None,), dtype=int32)\n","output_type":"stream"}]},{"cell_type":"code","source":"EPOCHS = 300","metadata":{"execution":{"iopub.status.busy":"2023-01-26T10:48:29.800076Z","iopub.execute_input":"2023-01-26T10:48:29.800454Z","iopub.status.idle":"2023-01-26T10:48:29.805311Z","shell.execute_reply.started":"2023-01-26T10:48:29.800413Z","shell.execute_reply":"2023-01-26T10:48:29.804010Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_ds, \n    validation_data=valid_ds,\n    epochs=EPOCHS \n)\n\n# Save model\nmodel.save_weights('/kaggle/working/transformer_chatbot.h5')","metadata":{"execution":{"iopub.status.busy":"2023-01-26T10:48:29.807065Z","iopub.execute_input":"2023-01-26T10:48:29.807455Z","iopub.status.idle":"2023-01-26T10:52:25.019707Z","shell.execute_reply.started":"2023-01-26T10:48:29.807420Z","shell.execute_reply":"2023-01-26T10:52:25.018332Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Epoch 1/2\nTensor(\"transformer/outputs/transformer_decoder_2/strided_slice_2:0\", shape=(None, 1), dtype=int32) Tensor(\"transformer/outputs/transformer_decoder_2/range_1:0\", shape=(None,), dtype=int32)\nTensor(\"transformer/outputs/transformer_decoder_2/strided_slice_2:0\", shape=(None, 1), dtype=int32) Tensor(\"transformer/outputs/transformer_decoder_2/range_1:0\", shape=(None,), dtype=int32)\n1125/1125 [==============================] - ETA: 0s - loss: 1.4892 - accuracy: 0.2146Tensor(\"transformer/outputs/transformer_decoder_2/strided_slice_2:0\", shape=(None, 1), dtype=int32) Tensor(\"transformer/outputs/transformer_decoder_2/range_1:0\", shape=(None,), dtype=int32)\n1125/1125 [==============================] - 92s 79ms/step - loss: 1.4892 - accuracy: 0.2146 - val_loss: 1.3678 - val_accuracy: 0.2385\nEpoch 2/2\n1125/1125 [==============================] - 91s 81ms/step - loss: 1.3606 - accuracy: 0.2413 - val_loss: 1.3355 - val_accuracy: 0.2458\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_weights('/kaggle/working/transformer_chatbot.h5')\n\nvocab = target_vectorizer.get_vocabulary()\nindex_lookup = dict(zip(range(len(vocab)), vocab))\n\ndef decode_sequence(input_sentence):\n    tokenized_input_sentence = input_vectorizer([input_sentence])\n    decoded_sentence = START_TOKEN\n    for i in range(OUTPUT_SEQUENCE_LENGTH):\n        tokenized_target_sentence = target_vectorizer([decoded_sentence])[:, :-1]\n        predictions = model([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = index_lookup[sampled_token_index]\n        decoded_sentence += ' ' + sampled_token\n\n        if sampled_token == STOP_TOKEN:\n            break\n    return decoded_sentence","metadata":{"execution":{"iopub.status.busy":"2023-01-26T10:56:08.626230Z","iopub.execute_input":"2023-01-26T10:56:08.626610Z","iopub.status.idle":"2023-01-26T10:56:08.726823Z","shell.execute_reply.started":"2023-01-26T10:56:08.626579Z","shell.execute_reply":"2023-01-26T10:56:08.725778Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def response(input_text):\n    input_sentence = input_text\n    translated = decode_sequence(input_sentence)\n    #print('-'*50)\n    print('Input: ', input_sentence)\n    print('Output: ', translated)","metadata":{"execution":{"iopub.status.busy":"2023-01-26T10:56:25.265680Z","iopub.execute_input":"2023-01-26T10:56:25.266076Z","iopub.status.idle":"2023-01-26T10:56:25.271508Z","shell.execute_reply.started":"2023-01-26T10:56:25.266041Z","shell.execute_reply":"2023-01-26T10:56:25.270492Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"response(\"I don't know\")","metadata":{"execution":{"iopub.status.busy":"2023-01-26T10:57:19.020270Z","iopub.execute_input":"2023-01-26T10:57:19.020632Z","iopub.status.idle":"2023-01-26T10:57:19.161696Z","shell.execute_reply.started":"2023-01-26T10:57:19.020599Z","shell.execute_reply":"2023-01-26T10:57:19.160654Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"tf.Tensor(\n[[ 0]\n [ 1]\n [ 2]\n [ 3]\n [ 4]\n [ 5]\n [ 6]\n [ 7]\n [ 8]\n [ 9]\n [10]\n [11]\n [12]\n [13]\n [14]\n [15]\n [16]\n [17]\n [18]\n [19]\n [20]\n [21]\n [22]\n [23]\n [24]\n [25]\n [26]\n [27]\n [28]\n [29]\n [30]\n [31]\n [32]\n [33]\n [34]\n [35]\n [36]\n [37]\n [38]\n [39]], shape=(40, 1), dtype=int32) tf.Tensor(\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39], shape=(40,), dtype=int32)\ntf.Tensor(\n[[ 0]\n [ 1]\n [ 2]\n [ 3]\n [ 4]\n [ 5]\n [ 6]\n [ 7]\n [ 8]\n [ 9]\n [10]\n [11]\n [12]\n [13]\n [14]\n [15]\n [16]\n [17]\n [18]\n [19]\n [20]\n [21]\n [22]\n [23]\n [24]\n [25]\n [26]\n [27]\n [28]\n [29]\n [30]\n [31]\n [32]\n [33]\n [34]\n [35]\n [36]\n [37]\n [38]\n [39]], shape=(40, 1), dtype=int32) tf.Tensor(\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39], shape=(40,), dtype=int32)\ntf.Tensor(\n[[ 0]\n [ 1]\n [ 2]\n [ 3]\n [ 4]\n [ 5]\n [ 6]\n [ 7]\n [ 8]\n [ 9]\n [10]\n [11]\n [12]\n [13]\n [14]\n [15]\n [16]\n [17]\n [18]\n [19]\n [20]\n [21]\n [22]\n [23]\n [24]\n [25]\n [26]\n [27]\n [28]\n [29]\n [30]\n [31]\n [32]\n [33]\n [34]\n [35]\n [36]\n [37]\n [38]\n [39]], shape=(40, 1), dtype=int32) tf.Tensor(\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39], shape=(40,), dtype=int32)\ntf.Tensor(\n[[ 0]\n [ 1]\n [ 2]\n [ 3]\n [ 4]\n [ 5]\n [ 6]\n [ 7]\n [ 8]\n [ 9]\n [10]\n [11]\n [12]\n [13]\n [14]\n [15]\n [16]\n [17]\n [18]\n [19]\n [20]\n [21]\n [22]\n [23]\n [24]\n [25]\n [26]\n [27]\n [28]\n [29]\n [30]\n [31]\n [32]\n [33]\n [34]\n [35]\n [36]\n [37]\n [38]\n [39]], shape=(40, 1), dtype=int32) tf.Tensor(\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39], shape=(40,), dtype=int32)\nInput:  I don't know?\nOutput:  [start] i don't know [stop]\n","output_type":"stream"}]},{"cell_type":"code","source":"response(\"Hello\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response(\"Do you want breakfast?\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response(\"Good morning\")","metadata":{},"execution_count":null,"outputs":[]}]}